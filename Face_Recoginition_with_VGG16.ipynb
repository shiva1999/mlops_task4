{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Face_Recoginition with VGG16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiva1999/mlops_task4/blob/master/Face_Recoginition_with_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfDfJMoUBgyw"
      },
      "source": [
        "# Face Recognition using VGG16\n",
        "\n",
        "### Loading the VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwFZbgunBgzp"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "#Loads the VGG16 model \n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1hI2W2NBgz7"
      },
      "source": [
        "### Inpsecting each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKgGcsQ1Bgz9"
      },
      "source": [
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r3yH8QGBg0E"
      },
      "source": [
        "### Freezing of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJQnNEfYBg0F"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "model = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCsKMOSbBg0M"
      },
      "source": [
        "### Let's make a function that returns our FC Head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17HRE1NEBg0N"
      },
      "source": [
        "def addTopModel(bottom_model, num_classes, D=256):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "    top_model = bottom_model.output\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\n",
        "    top_model = Dense(4096, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(1024, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(512, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(256, activation = \"relu\")(top_model)\n",
        "    top_model = Dense(32, activation = \"relu\")(top_model)\n",
        "    top_model = Dropout(0.3)(top_model)\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
        "    return top_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDdQqyoBg0b"
      },
      "source": [
        "model.input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6GKMf8UBg0l"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYKONB-_Bg0s"
      },
      "source": [
        "### Let's add our FC Head back onto VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "050KRlRZBg0t"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "FC_Head = addTopModel(model, num_classes)\n",
        "\n",
        "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
        "\n",
        "print(modelnew.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-_dfOwNBg00"
      },
      "source": [
        "### Loading our Face Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbuUJJ0mJ2It"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea13dHTABg01"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = 'drive/My Drive/wiki_crop/'\n",
        "validation_data_dir = 'drive/My Drive/a_test/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "batchsize = 32\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u_pWennBg1N"
      },
      "source": [
        "### Training our top layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcHr04HLBg1O",
        "outputId": "b3f1a273-b425-425b-f63e-e2462cec1436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"face_vgg.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          verbose = 1,\n",
        "                          patience= 3,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "modelnew.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(lr = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 2100 \n",
        "nb_validation_samples = 90\n",
        "epochs = 5\n",
        "train_batch_size = 32\n",
        "val_batch_size = 16\n",
        "\n",
        "history = modelnew.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // train_batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // val_batch_size)\n",
        "\n",
        "modelnew.save(\"face_vgg.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "65/65 [==============================] - 31s 469ms/step - loss: 0.4662 - accuracy: 0.7758 - val_loss: 1.1199 - val_accuracy: 0.5250\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.11989, saving model to face_vgg.h5\n",
            "Epoch 2/5\n",
            "65/65 [==============================] - 30s 458ms/step - loss: 0.4575 - accuracy: 0.7725 - val_loss: 0.7468 - val_accuracy: 0.4187\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.11989 to 0.74682, saving model to face_vgg.h5\n",
            "Epoch 3/5\n",
            "65/65 [==============================] - 29s 448ms/step - loss: 0.4439 - accuracy: 0.7879 - val_loss: 1.8926 - val_accuracy: 0.3562\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.74682\n",
            "Epoch 4/5\n",
            "65/65 [==============================] - 30s 463ms/step - loss: 0.4625 - accuracy: 0.7783 - val_loss: 1.1014 - val_accuracy: 0.4938\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.74682\n",
            "Epoch 5/5\n",
            "65/65 [==============================] - 29s 453ms/step - loss: 0.4462 - accuracy: 0.7884 - val_loss: 0.9454 - val_accuracy: 0.4187\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.74682\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}